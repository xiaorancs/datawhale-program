{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **机器学习入门笔记1**\n",
    "---------\n",
    "**问题1:机器学习、深度学习、AI的关系？**   \n",
    "\n",
    "机器学习涉及到多门学科，是人工智能领域的核心研究点。众所周知的深度学习只是机器学习的一个分支。  \n",
    "\n",
    "简单地来讲，机器学习就是从无序的数据中抽取有用的信息。  \n",
    "\n",
    "2007年12月IEEE数据挖掘的一篇论文总结了数据挖掘十大算法，分别是C4.5决策树、K-均值、支持向量机SVM、Apriori、最大期望算法EM、PageRank算法、Adaboost算法、K近邻算法KNN、朴素贝叶斯算法NB和分类回归树CART。 \n",
    "\n",
    "朴素贝叶斯是基于贝叶斯定力的机器学习分类器，假设输入数据的特征都是独立的，这通常被认为是一个很强的或者朴素的假设。逻辑回归则被认为是现代机器学习的hello world。面对一个数据集，数据科学家通常会采用logistics regression算法来初步熟悉手头的分类任务。  \n",
    "\n",
    "## 1.机器学习分类\n",
    "机器学习一般可分为有监督学习、无监督学习、半监督学习和强化学习。\n",
    "### 1.1有监督学习\n",
    "在有监督学习中，目标变量可分为标称型和数值型，前者常与分类任务相关，后者常与回归任务相关。分类或者是回归，除了决策树、支持向量机等算法之外，也包含众多深度学习的算法和模型，如VGG16网络、残差神经网络ResNet等。  \n",
    "\n",
    "在深度学习中，监督学习除了分类和回归，还产生了一下几种变体:  \n",
    "\n",
    "+ 序列生成:给定一张照片，预测描述图像的文字。序列生成有时可以被重新表示成一系列的分类问题。\n",
    "+ 语法树预测:给定一个句子，预测其分解生成的语法树\n",
    "+ 目标检测:给定一张图像，在图中特定目标周围生成一个边界框。这个问题可以看做是分类问题（给定多个边界框，对每个框内的目标进行分类），或者是分类与回归的联合问题(用向量回归来预测边界框的坐标)。\n",
    "+ 图像分割:给定一张图像，在特定物体上画一个像素级的掩模(mask)。  \n",
    "\n",
    "### 1.2无监督学习\n",
    "在无监督学习中，如果仅仅需要将数据划分为不同的离散的分组，属于聚类问题，而如果还需要判断数据与每个分组的相似程度，就属于密度估计问题。无监督学习中，常见的包括K-均值算法、最大期望算法、DBSCAN算法和Parzen窗设计等。\n",
    "### 1.3半监督学习\n",
    "半监督学习是有监督学习和无监督学习的结合体，强调的是不仅要学习属性间的结构关系，还要进一步进行模型分类预测。\n",
    "### 1.4强化学习(Reinforcement Learning)\n",
    "\n",
    "强化学习一直以来都被人们忽视，但是今年来google的deepmind公司将其成功应用于学习玩游戏以及下围棋等，开始受到大量的关注。  \n",
    "\n",
    "在强化学习中，智能体(agent)接收环境的信息，并学会选择时某种奖励最大化的行动。  \n",
    "\n",
    "## 2.机器学习模型\n",
    "机器学习=数据(data)+模型(model)+优化方法(optional strategy)\n",
    "   ### 2.1数据\n",
    "   **问题2:数据集的划分**\n",
    "   数据集一般被划分为训练集、测试集合验证集，即:在训练数据上训练模型，在验证数据上评估模型，一旦找到了最佳参数，在测试数据上测试模型性能。  \n",
    "   \n",
    "   这里常带有一个疑问:为什么不直接划分为训练集和测试集呢？  \n",
    "   \n",
    "   模型，特别是深度学习模型，在开发的过程中，需要设置层数以及每一层的大小(超参数，以便于模型参数(权重)分隔开)。调节超参数需要模型在验证集上性能作为反馈信号。  \n",
    "   \n",
    "   **问题3:如果可用数据很少，该如何处理**  \n",
    "   \n",
    "   当可用数据很少的时候，有三种经典的评估方法:简单的留出验证、K折验证以及带有打乱数据的重复K折验证  \n",
    "   \n",
    "   *2.1.1简单的留出验证*  \n",
    "   \n",
    "   留出一定比例的数据作为测试集。在剩余的数据集上训练模型，然后在测试集上评估模型。但是在这个过程中，会造成一个问题：每次基于模型在验证集上的性能来调节模型超参数，都会有一些关于验证数据的信息泄露到模型中，即信息泄露(information leak)。\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image2.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2.1.2K折验证*  \n",
    "   \n",
    "   将数据划分为大小相同的K个分区，对于每个分区i，在剩下的k-1个分区上训练模型，然后再分区i上评估模型。最终的分数等于k个分数的平均值。  \n",
    "   \n",
    "   对于相同的数据集，不同的训练集-测试集划分方法，如果模型性能变化很大，可以采用该方法。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image1.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "   *2.1.3 带有打乱数据的重复K折验证*  \n",
    "   \n",
    "   多次使用K折验证，在每次将数据划分为K个分区之前都把数据打乱，最终分数为每次K折验证分数的平均值。这种方法一共需要训练和评估P*K个模型，P是重复次数，计算代价很大。  \n",
    "   \n",
    "   \n",
    "   ### 2.2机器学习损失函数与优化方法\n",
    "   损失函数是衡量输出与预期值之间的距离，衡量算法在数据集上的性能，通过优化方法进行调整后，使得模型朝着正确的方向前进。  \n",
    "   \n",
    "   目前损失函数已经比较丰富了，对于二分类问题，常使用二元交叉熵(binary crossentropy)；对于多分类问题，常使用分类交叉熵(categorical crossentropy)；对于序列学习问题，常使用连接主义时序分类(CTS, connectionist temporal classification)损失函数等。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
